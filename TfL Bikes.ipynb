{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the journey data\n",
    "\n",
    "The TfL bike usage data is hosted as a number of CSV files on their website. I looped through each of these CSVs, aggregated them together into one dataset, and cleaned it all up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "## Read list of names of all files from a separate CSV - can't scrape them from website as they're hidden\n",
    "with open('urls_bike.csv', 'r') as f:\n",
    "    csv_list = f.read().splitlines()\n",
    "\n",
    "## Downloading all of the bike journey CSV files and appending to one dataset\n",
    "website = 'http://cycling.data.tfl.gov.uk/usage-stats/'\n",
    "\n",
    "url_list = [website + urllib.parse.quote(x) for x in csv_list]\n",
    "dfs = (pd.read_csv(url) for url in url_list)\n",
    "all_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(all_data.shape)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Clean data - NAs, and start/end being the same station, drop additional useless columns\n",
    "\n",
    "print(all_data.shape)\n",
    "\n",
    "all_data.dropna(axis=0, subset=[\"StartStation Id\", \"EndStation Id\", \"Start Date\", \"End Date\"], inplace=True)\n",
    "\n",
    "print(all_data.shape)\n",
    "\n",
    "all_data[\"EndStation Id\"] = all_data[\"EndStation Id\"].astype(int)\n",
    "all_data[\"StartStation Id\"] = all_data[\"StartStation Id\"].astype(int)\n",
    "\n",
    "all_data = all_data[all_data[\"StartStation Id\"] != all_data[\"EndStation Id\"]]\n",
    "\n",
    "all_data = all_data.loc[:,('Start Date',\n",
    "                           'StartStation Id',\n",
    "                           'End Date',\n",
    "                           'EndStation Id',\n",
    "                           'Duration')]\n",
    "                           \n",
    "print(all_data.shape)\n",
    "\n",
    "## Extra drop for duplicates\n",
    "\n",
    "all_data.drop_duplicates(inplace=True)\n",
    "print(all_data.shape)\n",
    "\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the bike station locations\n",
    "\n",
    "TfL have a live \"cycle hire updates\" feed which lists information for each cycle hire station, updated once every minute or so. I don't utilise this live data - instead I just take the name, ID, lat/lon, and capacity for each bike station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from xml.etree import ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "site = \"https://tfl.gov.uk/tfl/syndication/feeds/cycle-hire/livecyclehireupdates.xml\"\n",
    "\n",
    "response = requests.get(site)\n",
    "root = ET.fromstring(response.content)\n",
    "\n",
    "id_list = [int(root[i][0].text) for i in range(0, len(root))]\n",
    "name_list = [root[i][1].text for i in range(0, len(root))]\n",
    "lat_list = [float(root[i][3].text) for i in range(0, len(root))]\n",
    "lon_list = [float(root[i][4].text) for i in range(0, len(root))]\n",
    "capacity_list = [int(root[i][12].text) for i in range(0, len(root))]\n",
    "\n",
    "all_locs = pd.DataFrame(list(zip(name_list, id_list, lat_list, \n",
    "                                 lon_list, capacity_list)), columns = [\"name\",\"id\",\"lat\",\"lon\",\"capacity\"])\n",
    "\n",
    "all_locs.to_csv('bike_point_locations_saved.csv', header=True, index=None)\n",
    "\n",
    "print(all_locs.shape)\n",
    "all_locs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting all the bike stations in bokeh\n",
    "\n",
    "Once I've got all the bike station locations, I generate a quick interactive bokeh plot of all of them. For the backgrounds I use two separate shapefiles I downloaded - one of all the buildings in London, and one of all the roads. I've got a separate github repo which goes through how I configured all of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.models import GeoJSONDataSource, ColumnDataSource\n",
    "from bokeh.models.tools import PanTool, HoverTool, ResetTool, WheelZoomTool\n",
    "from bokeh.io import output_notebook, output_file, save, show\n",
    "from bokeh.sampledata.sample_geojson import geojson\n",
    "import bokeh.plotting as bp\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load both roads and buildings geojson files into correct format \n",
    "\n",
    "# Load buildings but truncate heavily to 100 polygons (at first)\n",
    "with open('Basemaps/London_buildings.geojson', 'r') as f:\n",
    "    geojson_buildings = f.read()\n",
    "    \n",
    "with open('Basemaps/London_roads.geojson', 'r') as f:\n",
    "    geojson_roads = f.read()\n",
    "\n",
    "# Load geojson\n",
    "json_buildings = GeoJSONDataSource(geojson=json.dumps(json.loads(geojson_buildings)))\n",
    "json_roads = GeoJSONDataSource(geojson=json.dumps(json.loads(geojson_roads)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort out the dataframe for plotting - bounding box, size of plot, etc\n",
    "\n",
    "all_locs = pd.read_csv('bike_point_locations_saved.csv')\n",
    "\n",
    "df_points = all_locs.loc[:,('name', 'capacity','lat', 'lon')]\n",
    "df_points['size'] = 5\n",
    "\n",
    "x_range = (df_points.lon.min() - 0.001, df_points.lon.max() + 0.003)\n",
    "y_range = (df_points.lat.min() - 0.003, df_points.lat.max() + 0.003)\n",
    "\n",
    "points_source = ColumnDataSource(ColumnDataSource.from_df(df_points))\n",
    "\n",
    "print(x_range)\n",
    "print(y_range)\n",
    "\n",
    "plot_h = 600\n",
    "plot_w = 900\n",
    "\n",
    "df_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Set up the bokeh plot. I use the \"hovertool\" feature from bokeh so that I can mouseover the points and get their\n",
    "## name and capacity. I had to play around with the circle renderer to get the hovertool to work properly, as \n",
    "## otherwise it interacts with the other elements in my bokeh plot and returns null values - i.e. if a bike point\n",
    "## sits on top of one of the road of building patches, then the hovertool was returning a null value. \n",
    "\n",
    "## This function seems to yield a bunch of GlyphRenderer errors, but the plot works exactly as intended...\n",
    "\n",
    "tools = [PanTool(), WheelZoomTool(), ResetTool()]\n",
    "\n",
    "p = bp.figure(tools=tools, plot_width=plot_w, plot_height=plot_h,\n",
    "    x_range=x_range, y_range=y_range, outline_line_color=None,\n",
    "    min_border=0, min_border_left=0, min_border_right=0,\n",
    "    min_border_top=0, min_border_bottom=0) \n",
    "\n",
    "p.patches(xs='xs', ys='ys', fill_alpha=0.3, fill_color='#0C090A',\n",
    "                   line_alpha=0, source=json_buildings)\n",
    "\n",
    "circles = p.circle(x='lon', y='lat', size='size', color='#ff0000', alpha=1, source=points_source)\n",
    "\n",
    "hover = HoverTool(\n",
    "        tooltips=[\n",
    "            (\"Name\", \"@name\"),\n",
    "            (\"Capacity\", \"@capacity\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "p.add_tools(hover)\n",
    "\n",
    "\n",
    "p.background_fill_color = '#2C3539'\n",
    "p.xaxis.visible = False\n",
    "p.yaxis.visible = False\n",
    "p.xgrid.grid_line_color = None\n",
    "p.ygrid.grid_line_color = None\n",
    "hover.renderers.append(circles)\n",
    "\n",
    "output_file(\"bokeh_plots/bike_points.html\")\n",
    "save(p)\n",
    "\n",
    "output_notebook()\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the usage data and location data\n",
    "\n",
    "I calculate the full set of unique routes actually made within the entire usage data, so that I can then run these unique routes through my journey planner. I get around 400k unique routes made - out of a total possible of around 600k (777 * 776)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Generate list of unique routes\n",
    "unq_locs = all_data.loc[:,('StartStation Id',\n",
    "                      'EndStation Id')]\n",
    "print(unq_locs.shape)\n",
    "unq_locs.drop_duplicates(inplace=True)\n",
    "print(unq_locs.shape)\n",
    "\n",
    "## Merge on the lat/lons\n",
    "\n",
    "unq_locs = unq_locs.merge(right = all_locs,\n",
    "                             how = 'inner',\n",
    "                             left_on = 'StartStation Id',\n",
    "                             right_on = 'id')\n",
    "\n",
    "print(unq_locs.shape)\n",
    "\n",
    "unq_locs.drop(labels = [\"id\", \"name\"], axis=1, inplace=True)\n",
    "unq_locs.rename(columns={'lat': 'StartStation lat', 'lon': 'StartStation lon', \n",
    "                            'capacity': 'StartStation capacity'},\n",
    "                   inplace=True)\n",
    "# Merge end\n",
    "unq_locs = unq_locs.merge(right = all_locs,\n",
    "                             how = 'inner',\n",
    "                             left_on = 'EndStation Id',\n",
    "                             right_on = 'id')\n",
    "\n",
    "unq_locs.drop(labels = [\"id\", \"name\"], axis=1, inplace=True)\n",
    "unq_locs.rename(columns={'lat': 'EndStation lat', 'lon': 'EndStation lon',\n",
    "                           'capacity': 'EndStation capacity'},\n",
    "                   inplace=True)\n",
    "\n",
    "\n",
    "print(unq_locs.shape)\n",
    "unq_locs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the predicted route for each journey taken\n",
    "\n",
    "I run each unique route through an OSRM server to calculate the predicted route taken, as well as its distance and expected duration. The result is a list of lat/lon waypoints for each route. \n",
    "\n",
    "DATASHADER NOTE - datashader's \"line\" drawing function takes a series of lat/lon points and draws a line between each one contiguously. Unfortunately, this means that it draws a line between the end of one journey and the start of the next, like if you were trying to draw a picture without taking your pen off the paper. To get around this, I append a null row at the end of every unique route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define functions to do routing requests to OSRM server - use asynchronous requests to speed it up\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import numpy as np\n",
    "concurrent = 500\n",
    "\n",
    "## Creating the URL for each individual route to send to the OSRM server\n",
    "def CreateUrl(row):\n",
    "    sid, slat, slon, eid, elat, elon = row\n",
    "    base_url = \"http://osmrouting.uksouth.cloudapp.azure.com/\"\n",
    "    route_url = \"osrm/route/v1/bicycle/{0},{1};{2},{3}\".format(slon, slat, elon, elat)\n",
    "    end_url = \"?alternatives=false&steps=false&geometries=geojson\"\n",
    "    return base_url+route_url+end_url, '%i_%i' % (sid, eid)\n",
    "\n",
    "url_list = unq_locs.apply(func=CreateUrl, axis=1)\n",
    "print(len(url_list))\n",
    "\n",
    "df_empty = pd.DataFrame(index=np.arange(0, 1), columns=('lat', 'lon', 'duration', 'distance'))\n",
    "\n",
    "## Handling the JSON request - need to insert an empty row in between each route \n",
    "def handle_req(data, qid):\n",
    "    r = json.loads(data.decode('utf-8'))\n",
    "    coord = r['routes'][0]['geometry']['coordinates']\n",
    "    distance = r['routes'][0]['distance']\n",
    "    duration = r['routes'][0]['duration']\n",
    "        \n",
    "    df_journey = pd.DataFrame(coord[:], columns=['lon', 'lat'])\n",
    "    df_journey['duration'] = duration\n",
    "    df_journey['distance'] = distance\n",
    "    df_journey = df_journey.append(df_empty)\n",
    "    \n",
    "    df_journey['route_id'] = qid\n",
    "\n",
    "    return df_journey\n",
    " \n",
    "## Asynchronous request functions\n",
    "def chunked_http_client(num_chunks, s):\n",
    "    # Use semaphore to limit number of requests\n",
    "    semaphore = asyncio.Semaphore(num_chunks)\n",
    "    @asyncio.coroutine\n",
    "    # Return co-routine that will work asynchronously and respect locking of semaphore\n",
    "    def http_get(url, qid):\n",
    "        nonlocal semaphore\n",
    "        with (yield from semaphore):\n",
    "            response = yield from s.get(url)\n",
    "            body = yield from response.content.read()\n",
    "            yield from response.wait_for_close()\n",
    "        return body, qid\n",
    "    return http_get\n",
    " \n",
    "def run_experiment(urls, _session):\n",
    "    http_client = chunked_http_client(num_chunks=concurrent, s=_session)\n",
    "    \n",
    "    # http_client returns futures, save all the futures to a list\n",
    "    tasks = [http_client(url, qid) for url, qid in urls]\n",
    "    \n",
    "    df_route = []\n",
    "    \n",
    "    # wait for futures to be ready then iterate over them\n",
    "    for future in asyncio.as_completed(tasks):\n",
    "        data, qid = yield from future\n",
    "        try:\n",
    "            out = handle_req(data, qid)\n",
    "            df_route.append(out)\n",
    "        except Exception as err:\n",
    "            print(\"Error for {0} - {1}\".format(qid, err))\n",
    "    return df_route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Send all of the requests to the OSRM server and compile the results together\n",
    "\n",
    "with aiohttp.ClientSession() as session: \n",
    "    loop = asyncio.get_event_loop()\n",
    "    calc_routes = loop.run_until_complete(run_experiment(url_list, session))\n",
    "    \n",
    "routes_complete = pd.concat(calc_routes, ignore_index=True)\n",
    "\n",
    "routes_complete.to_csv('all_id_routes.csv', header=True, index=None)\n",
    "\n",
    "print(routes_complete.shape)\n",
    "routes_complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering and collapsing the dataset\n",
    "\n",
    "Using the results from my predicted journeys, for the datashader plot I filter the dataset by only taking routes that took up to twice as long as predicted by the route planner (i.e. filtering out routes where the rider clearly went in an entirely different direction, or some very slow people).\n",
    "\n",
    "I also define a bunch of other functions below for filtering the dataset on e.g. time of day, removing weekends, collapsing by unique route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Reading the output of the above cells from the saved CSV file, then generating a separate dataset just of the \n",
    "## expected duration for each route\n",
    "\n",
    "routes_complete = pd.read_csv('all_id_routes.csv')\n",
    "\n",
    "routes_duration = routes_complete.loc[:,(\"route_id\",\"duration\")]\n",
    "print(routes_duration.shape)\n",
    "\n",
    "routes_duration.drop_duplicates(inplace=True)\n",
    "print(routes_duration.shape)\n",
    "\n",
    "## Dropping the NaN values\n",
    "routes_duration.dropna(axis=0,inplace=True)\n",
    "print(routes_duration.shape)\n",
    "\n",
    "routes_duration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Add on a route_id variable and use it to merge on the expected duration\n",
    "\n",
    "all_data['route_id'] = all_data['StartStation Id'].astype(str) + \"_\" + all_data['EndStation Id'].astype(str)\n",
    "\n",
    "print(all_data.shape)\n",
    "all_data = all_data.merge(right = routes_duration, how='inner', on='route_id')\n",
    "all_data.rename(columns={'duration': 'Expected Duration'},\n",
    "                   inplace=True)\n",
    "\n",
    "## Set upper limit at double the expected time for each unique route\n",
    "all_data['duration_upper'] = (2*all_data['Expected Duration'])\n",
    "\n",
    "print(all_data.shape)\n",
    "\n",
    "## Used for collapsing stuff later\n",
    "all_data['c'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Add some extra variables to the dataset for use later in filtering\n",
    "\n",
    "import datetime\n",
    "\n",
    "## Feeding a specififed date format speeds up the pd.to_datetime function immeasurably, especially over large datasets\n",
    "## e.g. http://stackoverflow.com/questions/32034689/why-is-pandas-to-datetime-slow-for-non-standard-time-format-such-as-2014-12-31\n",
    "\n",
    "format = \"%d/%m/%Y %H:%M\"\n",
    "\n",
    "## Some routes had dates with a seconds component, whereas some didn't - the below code cuts these seconds off\n",
    "all_data['Start Date']= all_data['Start Date'].str[:16]\n",
    "\n",
    "all_data['Start Date Converted']= pd.to_datetime(all_data['Start Date'], format=format).dt.date\n",
    "\n",
    "all_data['Hours']= pd.to_datetime(all_data['Start Date'], format=format).dt.hour\n",
    "\n",
    "all_data['Day']= pd.to_datetime(all_data['Start Date'], format=format).dt.weekday\n",
    "\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## All_routes is the subset where I cut out those over the expected threshold\n",
    "\n",
    "all_routes = all_data[all_data['Duration'] < all_data['duration_upper']].copy()\n",
    "\n",
    "del all_routes['duration_upper']\n",
    "del all_data['duration_upper']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This allows me to filter the dataset to between a specific time of day\n",
    "def time_filter(dataset, lower_time, upper_time):\n",
    "    dataset = dataset[(dataset['Hours'] >= lower_time) & (dataset['Hours'] < upper_time)]\n",
    "    return dataset\n",
    "\n",
    "## This does the same but for certain dates\n",
    "def date_filter(dataset, lower_year, lower_month, lower_day, upper_year, upper_month, upper_day):\n",
    "    dataset = dataset[(dataset[\"Start Date Converted\"] >= datetime.datetime(lower_year,lower_month,lower_day).date()) \n",
    "                      & (dataset[\"Start Date Converted\"] < datetime.datetime(upper_year,upper_month,upper_day).date())]\n",
    "    return dataset\n",
    "\n",
    "## This cuts out weekends \n",
    "def weekend_filter(dataset):\n",
    "    dataset = dataset[(dataset[\"Day\"] < 5)]\n",
    "    return dataset\n",
    "\n",
    "## This drops one specific day\n",
    "def drop_specific_date(dataset, year, month, day):\n",
    "    dataset = dataset[(dataset[\"Start Date Converted\"] != datetime.datetime(year,month,day).date())]\n",
    "    return dataset\n",
    "\n",
    "## This collapses down any cut of a dataset to get the count by route for each route_id\n",
    "def collapse_dataset(dataset, renamed_count):\n",
    "    dataset_collapse = dataset.loc[:, ('route_id', 'c')].groupby(['route_id']).sum()\n",
    "    dataset_collapse.reset_index(inplace=True)\n",
    "    dataset_collapse.rename(columns={'c': str(renamed_count)}, inplace=True)\n",
    "    return dataset_collapse\n",
    "\n",
    "## This collapses the dataset down and just looks and inflows and outflows for each station\n",
    "def inflow_outflow(dataset):\n",
    "    inflows = dataset.loc[:, ('EndStation Id', 'c')].copy().groupby(['EndStation Id']).sum()\n",
    "    inflows.reset_index(inplace=True)\n",
    "    inflows.rename(columns={'c': 'inflows', 'EndStation Id': 'id'}, inplace=True)\n",
    "\n",
    "    outflows = dataset.loc[:, ('StartStation Id', 'c')].copy().groupby(['StartStation Id']).sum()\n",
    "    outflows.reset_index(inplace=True)\n",
    "    outflows.rename(columns={'c': 'outflows', 'StartStation Id': 'id'}, inplace=True)\n",
    "    flows = inflows.merge(right = outflows, how='inner', on='id')\n",
    "    \n",
    "    flows['delta'] = flows['inflows'] - flows['outflows']\n",
    "\n",
    "    return flows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the entire dataset\n",
    "\n",
    "Using the functions above, I collapse the entire dataset down, merge it onto the waypoints for each route, then make a simple plot using datashader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Collapse down entire dataset, then merge onto the waypoints for each route\n",
    "\n",
    "print(all_routes.shape)\n",
    "\n",
    "collapsed_routes = collapse_dataset(all_routes, 'c_all')\n",
    "\n",
    "print(collapsed_routes.shape)\n",
    "\n",
    "agg_routes = collapsed_routes.merge(right = routes_complete, how='inner', on='route_id')\n",
    "\n",
    "print(agg_routes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot the resulting dataset using datashader, aggregating the lines by each route's count\n",
    "\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "from datashader.bokeh_ext import InteractiveImage\n",
    "\n",
    "df = agg_routes.loc[:,('lat', 'lon', 'c_all')]\n",
    "\n",
    "# Default plot ranges:\n",
    "\n",
    "x_range = (df.lon.min() - 0.001, df.lon.max() + 0.001)\n",
    "y_range = (df.lat.min() - 0.0005, df.lat.max() + 0.0005)\n",
    "\n",
    "plot_w = 900\n",
    "plot_h = 600\n",
    "\n",
    "background_tools = [PanTool(), WheelZoomTool(), ResetTool()]\n",
    "\n",
    "background = bp.figure(tools=background_tools, plot_width=plot_w, plot_height=plot_h,\n",
    "    x_range=x_range, y_range=y_range, outline_line_color=None,\n",
    "    min_border=0, min_border_left=0, min_border_right=0,\n",
    "    min_border_top=0, min_border_bottom=0) \n",
    "\n",
    "background.patches(xs='xs', ys='ys', fill_alpha=0.3, fill_color='#0C090A',\n",
    "                   line_alpha=0, source=json_buildings)\n",
    "\n",
    "background.background_fill_color = '#2C3539'\n",
    "background.xaxis.visible = False\n",
    "background.yaxis.visible = False\n",
    "background.xgrid.grid_line_color = None\n",
    "background.ygrid.grid_line_color = None\n",
    "\n",
    "\n",
    "def create_image(x_range=x_range, y_range=y_range, w=plot_w, h=plot_h):\n",
    "    cvs = ds.Canvas(x_range=x_range, y_range=y_range, plot_height=h, plot_width=w)\n",
    "    agg = cvs.line(df, 'lon', 'lat', agg=ds.count('c_all'))\n",
    "    return tf.shade(agg, cmap=[\"darkred\", \"red\"])\n",
    "\n",
    "InteractiveImage(background, create_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS\n",
    "\n",
    "Now that I have a completed dataset, I can make various cuts to the dataset to see commuting patterns and the effects of tube strikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Dataset of just the names of stations\n",
    "\n",
    "all_data_names = all_locs.loc[:, (['id', 'name'])].copy()\n",
    "all_data_names.rename(columns={'name': 'Name'}, inplace=True)\n",
    "all_data_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Look at most bikes taken across entire dataset\n",
    "\n",
    "all_flows = inflow_outflow(all_data)\n",
    "\n",
    "all_flows['Total Flows'] = all_flows['inflows'] + all_flows['outflows']\n",
    "\n",
    "all_flows.sort_values('Total Flows', axis=0, inplace=True, ascending=False)\n",
    "\n",
    "all_flows = all_flows.merge(right = all_data_names, how='left', on='id')\n",
    "\n",
    "all_flows = all_flows[['Name', 'Total Flows']][:10]\n",
    "\n",
    "all_flows[:10]\n",
    "\n",
    "all_flows.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Can also use the PageRank algorithm \n",
    "import networkx as nx\n",
    "\n",
    "data_for_rank = all_data.loc[:, ('StartStation Id', 'EndStation Id', 'c')].groupby(['StartStation Id', 'EndStation Id']).sum()\n",
    "data_for_rank.reset_index(inplace=True)\n",
    "data_for_rank.rename(columns={'c': 'Count'}, inplace=True)\n",
    "\n",
    "data_for_rank.head()\n",
    "\n",
    "network = nx.from_pandas_dataframe(data_for_rank, 'StartStation Id', 'EndStation Id', edge_attr=['Count'])\n",
    "\n",
    "pagerank = pd.DataFrame(list(nx.pagerank(network).items()), columns = ['id', 'Rank'])\n",
    "\n",
    "pagerank = pagerank.merge(right=all_data_names, how='left', on='id')\n",
    "\n",
    "pagerank.sort_values('Rank', axis=0, inplace=True, ascending=False)\n",
    "pagerank.reset_index(inplace=True, drop=True)\n",
    "\n",
    "pagerank = pagerank.loc[:, ('Name', 'Rank')]\n",
    "\n",
    "pagerank[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Looking at stations with highest redistribution\n",
    "\n",
    "redistribution = inflow_outflow(all_data)\n",
    "redistribution.sort_values('delta', axis=0, inplace=True)\n",
    "redistribution.reset_index(inplace=True, drop=True)\n",
    "\n",
    "redistribution = redistribution.merge(right=all_data_names, how='left', on='id')\n",
    "redistribution = redistribution.loc[:, (['Name', 'delta'])]\n",
    "redistribution.rename(columns={'delta': 'Inflows - Outflows'}, inplace=True)\n",
    "\n",
    "redistribution[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at mornings vs evenings across the dataset to see different flows\n",
    "\n",
    "Would expect to see people commuting into the centre in the morning, and back out to the suburbs in the evenings. Cut the dataset by morning/evening, filter out weekends, then produce an interactive plot of morning vs evening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Filter dataset to only look at mornings and evenings to see commuting patterns\n",
    "\n",
    "all_mornings = weekend_filter(all_data)\n",
    "all_mornings = time_filter(all_mornings, 7, 10)\n",
    "\n",
    "all_evenings = weekend_filter(all_data)\n",
    "all_evenings = time_filter(all_evenings, 17, 20)\n",
    "\n",
    "all_mornings_flows = inflow_outflow(all_mornings)\n",
    "all_evenings_flows = inflow_outflow(all_evenings)\n",
    "\n",
    "all_mornings_flows.rename(columns={'delta': 'mornings'}, inplace=True)\n",
    "all_evenings_flows.rename(columns={'delta': 'evenings'}, inplace=True)\n",
    "\n",
    "mornings_evenings = all_mornings_flows.merge(right=all_evenings_flows, on='id', how='inner')\n",
    "\n",
    "## Below function sets colour as red if more inflows than outflows, and green if outflows > inflows\n",
    "mornings_evenings['c_mornings'] = mornings_evenings.apply(lambda x: 'red' if x['mornings'] < 0 else 'green', axis=1)\n",
    "mornings_evenings['c_evenings'] = mornings_evenings.apply(lambda x: 'red' if x['evenings'] < 0 else 'green', axis=1)\n",
    "\n",
    "mornings_evenings = mornings_evenings[['id', 'c_mornings', 'c_evenings']]\n",
    "\n",
    "mornings_evenings = mornings_evenings.merge(right=all_locs, how='inner', on='id')\n",
    "\n",
    "mornings_evenings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Set up bokeh plot\n",
    "\n",
    "df_diff = mornings_evenings.loc[:,('name','lat', 'lon', 'c_mornings', 'c_evenings')]\n",
    "df_diff['size'] = 5\n",
    "\n",
    "x_range = (df_diff.lon.min() - 0.001, df_diff.lon.max() + 0.003)\n",
    "y_range = (df_diff.lat.min() - 0.003, df_diff.lat.max() + 0.003)\n",
    "\n",
    "points_source = ColumnDataSource(ColumnDataSource.from_df(df_diff))\n",
    "\n",
    "print(x_range)\n",
    "print(y_range)\n",
    "\n",
    "plot_h = 600\n",
    "plot_w = 900\n",
    "\n",
    "df_diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Bokeh plot - using panels to flip between mornings and evenings\n",
    "\n",
    "## Get GlyphRenderer errors again, but can ignore those as it still works fine\n",
    "\n",
    "from bokeh.models.widgets import Panel, Tabs\n",
    "\n",
    "tools1 = [PanTool(), WheelZoomTool(), ResetTool()]\n",
    "\n",
    "p1 = bp.figure(tools=tools1, plot_width=plot_w, plot_height=plot_h,\n",
    "    x_range=x_range, y_range=y_range, outline_line_color=None,\n",
    "    min_border=0, min_border_left=0, min_border_right=0,\n",
    "    min_border_top=0, min_border_bottom=0) \n",
    "\n",
    "p1.patches(xs='xs', ys='ys', fill_alpha=0, \n",
    "              line_color = '#b3b3b3', line_alpha=0.3, source=json_roads)\n",
    "\n",
    "circles1 = p1.circle(x='lon', y='lat', size='size', color='c_mornings', alpha=1, source=points_source)\n",
    "\n",
    "hover1 = HoverTool(\n",
    "        tooltips=[\n",
    "            (\"Name\", \"@name\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "p1.add_tools(hover1)\n",
    "p1.background_fill_color = '#000000'\n",
    "p1.xaxis.visible = False\n",
    "p1.yaxis.visible = False\n",
    "p1.xgrid.grid_line_color = None\n",
    "p1.ygrid.grid_line_color = None\n",
    "hover1.renderers.append(circles1)\n",
    "\n",
    "tab1 = Panel(child=p1, title=\"Mornings\")\n",
    "\n",
    "\n",
    "tools2 = [PanTool(), WheelZoomTool(), ResetTool()]\n",
    "\n",
    "p2 = bp.figure(tools=tools2, plot_width=plot_w, plot_height=plot_h,\n",
    "    x_range=x_range, y_range=y_range, outline_line_color=None,\n",
    "    min_border=0, min_border_left=0, min_border_right=0,\n",
    "    min_border_top=0, min_border_bottom=0) \n",
    "\n",
    "p2.patches(xs='xs', ys='ys', fill_alpha=0, \n",
    "              line_color = '#b3b3b3', line_alpha=0.3, source=json_roads)\n",
    "\n",
    "circles2 = p2.circle(x='lon', y='lat', size='size', color='c_evenings', alpha=1, source=points_source)\n",
    "\n",
    "hover2 = HoverTool(\n",
    "        tooltips=[\n",
    "            (\"Name\", \"@name\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "p2.add_tools(hover2)\n",
    "\n",
    "p2.background_fill_color = '#000000'\n",
    "p2.xaxis.visible = False\n",
    "p2.yaxis.visible = False\n",
    "p2.xgrid.grid_line_color = None\n",
    "p2.ygrid.grid_line_color = None\n",
    "hover2.renderers.append(circles2)\n",
    "\n",
    "tab2 = Panel(child=p2, title=\"Evenings\")\n",
    "\n",
    "tabs = Tabs(tabs=[ tab1, tab2 ])\n",
    "\n",
    "output_notebook()\n",
    "show(tabs)\n",
    "\n",
    "output_file(\"bokeh_plots/mornings_evenings.html\")\n",
    "save(tabs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tube strike analysis\n",
    "\n",
    "Looking at one particular tube strike - see if it has an impact on how many journeys taken etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Tube strike - 8th July 2015 18:30 until 9th July 2015 21:30\n",
    "\n",
    "tube_strike = time_filter(all_data, 7, 10)\n",
    "tube_strike = date_filter(tube_strike, 2015,7,9,2015,7,10)\n",
    "\n",
    "## First control group - a week before the strike on the same day\n",
    "\n",
    "tube_strike_control_1 = time_filter(all_data, 7, 10)\n",
    "tube_strike_control_1 = date_filter(tube_strike_control_1, 2015,7,2,2015,7,3)\n",
    "\n",
    "## Second control group - a week after the strike on the same day\n",
    "\n",
    "tube_strike_control_2 = time_filter(all_data, 7, 10)\n",
    "tube_strike_control_2 = date_filter(tube_strike_control_2, 2015,7,16,2015,7,17)\n",
    "\n",
    "## Larger control group - month either side of the strike, dropping weekends and strike itself - 43 days total\n",
    "\n",
    "control = time_filter(all_data, 7, 10)\n",
    "control = date_filter(control, 2015,6,9,2015,8,9)\n",
    "control = weekend_filter(control)\n",
    "control = drop_specific_date(control, 2015,7,9)\n",
    "\n",
    "## Two month period including tube strike\n",
    "\n",
    "period = date_filter(all_data, 2015,6,9,2015,8,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Number of journeys made on tube strike morning\n",
    "\n",
    "print(tube_strike.shape)\n",
    "\n",
    "## Number of journeys week before\n",
    "\n",
    "print(tube_strike_control_1.shape)\n",
    "\n",
    "## Number of journeys week after\n",
    "\n",
    "print(tube_strike_control_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Bar chart of number of journeys per day, including strike day\n",
    "\n",
    "from bokeh.charts import Bar\n",
    "\n",
    "period_collapse = period.loc[:, ('Start Date Converted', 'c')].groupby(['Start Date Converted']).sum()\n",
    "period_collapse.reset_index(inplace=True)\n",
    "period_collapse.rename(columns={'c': 'Count', 'Start Date Converted': 'Date'}, inplace=True)\n",
    "\n",
    "period_collapse.head()\n",
    "\n",
    "tools = [PanTool(), WheelZoomTool(), ResetTool()]\n",
    "\n",
    "bar = Bar(period_collapse, values='Count', label='Date', color='red', ylabel='Count',\n",
    "          legend = None, title=\"Number of journeys per day\", plot_width=950, tools=tools)\n",
    "\n",
    "output_notebook()\n",
    "show(bar)\n",
    "\n",
    "output_file(\"bokeh_plots/journeys_per_day.html\")\n",
    "save(bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Statistical test to see if number of journeys on strike day is significantly different from control group average\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "control_collapse = control.loc[:, ('Start Date Converted', 'c')].groupby(['Start Date Converted']).sum()\n",
    "control_collapse.reset_index(inplace=True)\n",
    "control_collapse.rename(columns={'c': 'Count', 'Start Date Converted': 'Date'}, inplace=True)\n",
    "\n",
    "print(control_collapse['Count'].mean())\n",
    "print(control_collapse['Count'].std())\n",
    "\n",
    "print(tube_strike['c'].sum())\n",
    "\n",
    "x_bar = control_collapse['Count'].mean()\n",
    "mu_0 = tube_strike['c'].sum()\n",
    "s = control_collapse['Count'].std()\n",
    "n = 43\n",
    "\n",
    "t_stat = (x_bar - mu_0) / (s / sqrt(n))\n",
    "\n",
    "print(t_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Show difference in average number of flows in control vs number on tube strike - for top 10 routes \n",
    "\n",
    "tube_strike_flows = inflow_outflow(tube_strike)\n",
    "\n",
    "tube_strike_flows['Flows'] = (tube_strike_flows['inflows'] + tube_strike_flows['outflows'])\n",
    "\n",
    "tube_strike_flows = tube_strike_flows[['id', 'Flows']]\n",
    "\n",
    "control_flows = inflow_outflow(control)\n",
    "\n",
    "control_flows['Flows'] = (control_flows['inflows'] + control_flows['outflows']) / 43\n",
    "\n",
    "control_flows = control_flows[['id', 'Flows']]\n",
    "\n",
    "control_flows.sort_values('Flows', axis=0, inplace=True, ascending=False)\n",
    "\n",
    "control_flows = control_flows[:10]\n",
    "control_flows = control_flows.merge(right=all_data_names, how='left', on='id')\n",
    "\n",
    "tube_strike_flows = tube_strike_flows.merge(right=control_flows, how='right', on = 'id')\n",
    "tube_strike_flows.rename(columns={'Flows_x': 'Flows'}, inplace=True)\n",
    "del tube_strike_flows['Flows_y']\n",
    "\n",
    "tube_strike_flows['Type'] = \"Tube Strike\"\n",
    "control_flows['Type'] = \"Control\"\n",
    "\n",
    "tube_strike_flows\n",
    "\n",
    "flows = control_flows.append(tube_strike_flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Bar chart of number of flows for top routes\n",
    "\n",
    "tools = [PanTool(), WheelZoomTool(), ResetTool()]\n",
    "\n",
    "bar = Bar(flows, values='Flows', label='Name', group='Type', color=['green', 'red'], ylabel='Flows',\n",
    "          legend = 'top_right', title=\"Flows\", plot_width=950, tools=tools)\n",
    "\n",
    "output_notebook()\n",
    "show(bar)\n",
    "\n",
    "output_file(\"bokeh_plots/top_stations.html\")\n",
    "save(bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tube_strike_hist = tube_strike[['Duration']]\n",
    "tube_strike_hist = tube_strike_hist[tube_strike_hist['Duration'] < 6000]\n",
    "tube_strike_hist['Type'] = \"Tube Strike\"\n",
    "\n",
    "control_hist = control[['Duration']]\n",
    "control_hist = control_hist[control_hist['Duration'] < 6000]\n",
    "control_hist['Type'] = \"Control\"\n",
    "\n",
    "hist = tube_strike_hist.append(control_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Look at distribution of duration\n",
    "\n",
    "from bokeh.charts import Histogram\n",
    "from bokeh.layouts import gridplot\n",
    "\n",
    "hist1 = Histogram(control_hist, values='Duration', ylabel=\"Flows (Control)\", xlabel=\"\",\n",
    "                  bins=100, width=800, height=300, color=\"green\")\n",
    "hist2 = Histogram(tube_strike_hist, values='Duration', ylabel=\"Flows (Tube Strike)\", xlabel=\"Duration (seconds)\",\n",
    "                  bins=100, width=800, height=300, color=\"red\")\n",
    "\n",
    "\n",
    "p = gridplot([[hist1], [hist2]], toolbar_location=\"above\")\n",
    "\n",
    "output_notebook()\n",
    "show(p)\n",
    "\n",
    "output_file(\"bokeh_plots/histogram.html\")\n",
    "save(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Formal two-sample Kolmogorov-Smirnov test\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "control_array = control[['Duration']].values[:, 0]\n",
    "tube_strike_array = tube_strike[['Duration']].values[:, 0]\n",
    "\n",
    "stats.ks_2samp(control_array, tube_strike_array)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
